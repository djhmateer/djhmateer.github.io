---
layout: post
# title: Pull Requests 
description: 
menu: review
categories: s3 
published: true 
comments: false     
sitemap: false
image: /assets/2024-05-03/4.jpg
---

<!-- [![alt text](/assets/2025-08-30/6.jpg "Volcano")](/assets/2025-08-30/6.jpg) -->

I've used [Cyberduck](https://cyberduck.io/) for years as an S3 viewer for files in Digital Ocean spaces. The main problem has been selecting a large number of files and downloading them.

## S3 Browser

[https://s3browser.com/](https://s3browser.com/)

Download To - is not avaiable in the free version.


## rclone

[https://rclone.org/](https://rclone.org/)

This is a command line tool (and gui) which I'm using from WSL2 on Windows.

```bash
# 1.71.0 on 15th Sept 25
sudo -v ; curl https://rclone.org/install.sh | sudo bash

# gui is the web browser (but I prefer command line)
rclone rcd --rc-web-gui

#~/config/rclone/rclone.conf
# Digital Ocean Frankfurt (my default)
[do-first-project]
type = s3
provider = DigitalOcean
env_auth = false
access_key_id = XXXXXX
secret_access_key = XXXXX
region = fra1
endpoint = fra1.digitaloceanspaces.com

# Digital Ocean New York
[do-first-project-nyc3]
type = s3
provider = DigitalOcean
env_auth = false
access_key_id = XXX
secret_access_key = XXXXX 
region = nyc3
endpoint = nyc3.digitaloceanspaces.com

# Hetzner Falkenstein
# will be writing to... notice not public but can make public on Hetzner UI
[hetzner-fsn1]
type = s3
provider = Other
access_key_id = XXXXX 
secret_access_key = XXXX 
endpoint = fsn1.your-objectstorage.com
region = fsn1

# list bucket davetesting
rclone lsd do-first-project:davetesting

# copy (doesn't delete any extra files at destination)
rclone copy do-first-project:davetesting /mnt/f/Backups/DigitalOcean/davetesting --progress

# make sure destination folder exists (will create automatically)
# 8.6GB test - a single 4GB file in there
# doing 4 files at a time
# maxing out my USB HDD

# it wont copy again if file is already there (looking at time within 2secs worked for me)
rclone copy do-first-project:autoarchiverfeatures /mnt/f/Backups/DigitalOcean/autoarchiverfeatures --progress --modify-window 2s

# 40k files
# fast-list if 10k more files - gets a flat list. Less API requests. More RAM usage locally.
rclone copy do-first-project:aademomain /mnt/f/Backups/DigitalOcean/aademomain --progress --modify-window 2s --fast-list

rclone copy do-first-project:lighthouse-reports /mnt/f/Backups/DigitalOcean/lighthouse-reports --progress --modify-window 2s --fast-list

rclone copy do-first-project-nyc3:carlos-demo /mnt/f/Backups/DigitalOcean/carlos-demo --progress --modify-window 2s --fast-list

rclone copy do-first-project:glan-ytbm /mnt/f/Backups/DigitalOcean/glan-ytbm --progress --modify-window 2s --fast-list

# I got NoSuchKey errors with fast-list, but fine without (on subsequent checks)
rclone copy do-first-project-nyc3:pluro /mnt/f/Backups/DigitalOcean/pluro --progress --modify-window 2s --fast-list

rclone copy do-first-project:glan /mnt/f/Backups/DigitalOcean/glan --progress --modify-window 2s --fast-list

## Hetzner
fsn1.your-objectstorage.com

rclone lsd hetzner-fsn1:davetesting

# copy from local to Hetzner
rclone copy /mnt/f/Backups/DigitalOcean/davetesting hetzner-fsn1:davetesting  --progress
```



## Conclusion

I'm loving the stability of rclone 
